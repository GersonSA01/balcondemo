# ğŸ“Š ANÃLISIS EXHAUSTIVO DEL PROYECTO CHATBOT RAG UNEMI

## ğŸ—ï¸ ARQUITECTURA GENERAL

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        FRONTEND (Svelte)                     â”‚
â”‚  - ChatbotInline.svelte (UI principal)                       â”‚
â”‚  - GestiÃ³n de estado, historial, formularios                 â”‚
â”‚  - WebSocket/Fetch API â†’ Backend Django                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼ HTTP POST /api/chat/
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     BACKEND (Django + Python)                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  views.py â†’ chat_api()                              â”‚    â”‚
â”‚  â”‚  â”œâ”€ Recibe: message, history, category, student_dataâ”‚    â”‚
â”‚  â”‚  â””â”€ Llama: classify_with_rag()                      â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                â”‚                                              â”‚
â”‚                â–¼                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  rag_chat_service.py (ORQUESTADOR PRINCIPAL)       â”‚    â”‚
â”‚  â”‚  â”œâ”€ Maneja 4 stages: ready â†’ await_confirm â†’ RAG   â”‚    â”‚
â”‚  â”‚  â”œâ”€ 4 Niveles de confianza para responder          â”‚    â”‚
â”‚  â”‚  â””â”€ Handoff automÃ¡tico si no hay info              â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                â”‚                                              â”‚
â”‚                â”œâ”€â”€â†’ intent_parser.py (Interpreta intenciÃ³n)  â”‚
â”‚                â”œâ”€â”€â†’ conversation_context.py (Contexto)       â”‚
â”‚                â”œâ”€â”€â†’ hierarchical_router.py (Router)          â”‚
â”‚                â”œâ”€â”€â†’ pdf_responder.py (Genera respuesta)      â”‚
â”‚                â”œâ”€â”€â†’ answerability.py (Score confianza)       â”‚
â”‚                â”œâ”€â”€â†’ handoff.py (DecisiÃ³n de derivaciÃ³n)      â”‚
â”‚                â””â”€â”€â†’ taxonomy.py (ClasificaciÃ³n)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‚ ESTRUCTURA DE ARCHIVOS

### Backend (Django)

```
app/
â”œâ”€â”€ services/                    # LÃ³gica de negocio (8 mÃ³dulos)
â”‚   â”œâ”€â”€ config.py               # Cliente LLM, umbrales, feature flags
â”‚   â”œâ”€â”€ rag_chat_service.py     # ORQUESTADOR PRINCIPAL (691 lÃ­neas)
â”‚   â”œâ”€â”€ intent_parser.py        # Parser de intenciones con LLM
â”‚   â”œâ”€â”€ conversation_context.py # Contexto conversacional (234 lÃ­neas)
â”‚   â”œâ”€â”€ pdf_responder.py        # GeneraciÃ³n de respuestas RAG (208 lÃ­neas)
â”‚   â”œâ”€â”€ answerability.py        # Juez de respondibilidad (155 lÃ­neas)
â”‚   â”œâ”€â”€ handoff.py              # LÃ³gica de derivaciÃ³n (403 lÃ­neas)
â”‚   â”œâ”€â”€ taxonomy.py             # Mapeo a categorÃ­as (78 lÃ­neas)
â”‚   â”œâ”€â”€ retriever.py            # Retriever hÃ­brido (319 lÃ­neas)
â”‚   â”œâ”€â”€ query_planner.py        # Query understanding (274 lÃ­neas)
â”‚   â”œâ”€â”€ hierarchical_router.py  # Router jerÃ¡rquico (197 lÃ­neas)
â”‚   â””â”€â”€ title_lexicon.py        # Ãndice de tÃ­tulos (243 lÃ­neas)
â”‚
â”œâ”€â”€ views.py                    # Endpoints API (343 lÃ­neas)
â”œâ”€â”€ urls.py                     # Routing de URLs
â””â”€â”€ data/                       # Base de conocimiento
    â”œâ”€â”€ unemi_interno/          # PDFs internos UNEMI
    â”‚   â”œâ”€â”€ estudiantes/        # Reglamentos estudiantes
    â”‚   â””â”€â”€ tic/                # GestiÃ³n SGA, polÃ­ticas TIC
    â”œâ”€â”€ legal_nacional/         # Leyes y cÃ³digos
    â””â”€â”€ epunemi/                # Certificados EPUNEMI
```

### Frontend (Svelte)

```
frontend/src/
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ ChatbotInline.svelte    # Componente principal (600 lÃ­neas)
â”‚   â”œâ”€â”€ stores/                 # State management
â”‚   â””â”€â”€ balcon/                 # Otros servicios
â””â”€â”€ main.js                     # Entry point
```

---

## ğŸ”„ FLUJO COMPLETO DE UNA CONSULTA

### Paso 1: Usuario envÃ­a mensaje

```javascript
// ChatbotInline.svelte
async function processMessage(text) {
  const requestBody = { 
    message: text,
    history: formatHistoryForBackend(),
    category: currentCategory,
    subcategory: currentSubcategory,
    student_data: studentData
  };
  
  fetch('/api/chat/', { method: 'POST', body: JSON.stringify(requestBody) })
}
```

### Paso 2: Backend recibe y procesa

```python
# views.py â†’ chat_api()
def chat_api(request):
    text = payload.get("message")
    conversation_history = payload.get("history", [])
    category = payload.get("category")
    student_data = payload.get("student_data")
    
    # Llamar al orquestador principal
    result = classify_with_rag(text, conversation_history, category, subcategory, student_data)
```

### Paso 3: Orquestador RAG (rag_chat_service.py)

El orquestador maneja **3 stages** y **4 niveles de confianza**:

#### **Stage 1: ready** (Primera interacciÃ³n)

```python
if stage == "ready":
    # Interpretar intenciÃ³n del usuario
    intent_slots = interpretar_intencion_principal(user_text)  # LLM #1
    
    # Pedir confirmaciÃ³n
    return {
        "needs_confirmation": True,
        "summary": "Â¿Confirmas que quieres solicitar X?",
        "intent_slots": intent_slots
    }
```

**Llamadas LLM: 1**

---

#### **Stage 2: await_confirm** (Usuario confirma)

```python
if stage == "await_confirm":
    if es_confirmacion_positiva(user_text):
        # ===== AQUÃ EMPIEZA EL ANÃLISIS COMPLETO =====
        
        # 1. CONTEXTO CONVERSACIONAL (si aplica)
        context_evaluation = should_use_conversational_mode(user_text, history)  # LLM #2
        
        if context_evaluation["needs_context"]:
            enriched_text = enrich_query_with_context(user_text, history)  # LLM #3
            intent_slots = interpretar_intencion_principal(enriched_text)  # LLM #4
        
        # 2. ROUTING JERÃRQUICO
        hierarchical_cands = hierarchical_candidates(user_text)
        # â†’ Carpetas: ['unemi_interno/estudiantes', 'unemi_interno/tic']
        # â†’ Files: 4 PDFs candidatos
        
        # 3. QUERY UNDERSTANDING
        canon_q = _canonicalize_query(intent_query)
        entities = detect_entities(user_text)
        
        # 4. QUERY PLANNER
        planned_queries = plan_queries(intent_slots, canon_q, user_text)
        # â†’ ["Solicitar cambio de paralelo...", "cambio paralelo"]
        
        # 5. MULTI-STAGE RETRIEVAL (ITERATIVO)
        for query in planned_queries[:3]:  # MÃ¡ximo 3 queries
            
            # 5.1 Answerability Score
            ascore = answerability_score(query, retriever, k=12)  # LLM #5 (veredicto)
            # â†’ {confidence: 0.600, verdict: "yes", non_empty_docs: 8}
            
            # 5.2 Si confianza baja, intentar variantes LLM
            if ascore["confidence"] < 0.65:
                variants = gen_query_variants_llm(query, n=4)  # LLM #6
                # â†’ ["Â¿CÃ³mo puedo solicitar cambio de secciÃ³n?", ...]
                
                for variant in variants:
                    variant_score = answerability_score(variant, retriever)
                    # Usar la mejor variante
        
        # 5.3 RRF Fusion de documentos
        fused_docs = rrf_fuse([docs_q1, docs_q2, docs_q3])
        
        # 6. DECISIÃ“N SEGÃšN CONFIANZA
        
        # === NIVEL 1: Alta confianza (â‰¥ 0.70) ===
        if ascore["confidence"] >= TAU_NORMA:
            result = responder_desde_pdfs(intent_query, docs=fused_docs)  # LLM #7
            # â†’ Respuesta con JSON: {has_information, confidence, answer}
            
            return {
                "summary": result["respuesta"],
                "confidence": ascore["confidence"],
                "handoff": False
            }
        
        # === NIVEL 3: Confianza media (0.42-0.70) ===
        if ascore["confidence"] >= TAU_MIN:
            result = responder_desde_pdfs(intent_query, docs=fused_docs)  # LLM #8
            
            # Auto-evaluaciÃ³n del LLM
            if result["has_information"]:  # LLM dice que SÃ tiene info
                
                # Verificar si es intenciÃ³n crÃ­tica OBLIGATORIA
                if intent_short in INTENCIONES_CRITICAS_OBLIGATORIAS:
                    # â†’ Ir a Nivel 4 (derivar)
                    pass
                else:
                    # â†’ Responder con la info
                    return {
                        "summary": result["respuesta"],
                        "confidence": 0.5,
                        "handoff": False
                    }
        
        # === NIVEL 4: No hay info (< 0.42 O intenciÃ³n crÃ­tica) ===
        
        # Recuperar texto ORIGINAL de la consulta (no el "sÃ­")
        original_query = <recuperar del historial>
        
        # Evaluar handoff con clasificaciÃ³n LLM
        handoff_decision = should_handoff(
            confidence=ascore["confidence"],
            intent_short=intent_short,
            user_text=original_query  # LLM #9 (classify_with_llm)
        )
        # â†’ {handoff: True, channel: "Mesa de Ayuda AcadÃ©mica", department: "acadÃ©mico"}
        
        # Clasificar con taxonomÃ­a LLM
        mapping = map_to_taxonomy(intent_query)  # LLM #10
        # â†’ {categoria: "AcadÃ©mico", subcategoria: "Cambios"}
        
        # Mensaje de derivaciÃ³n
        respuesta_final = (
            f"{nombre}, no encontrÃ© informaciÃ³n especÃ­fica...\n\n"
            f"âœ… He derivado tu solicitud a {channel} {emoji}\n\n"
            f"ğŸ“§ Mantente atento a tu correo..."
        )
        
        return {
            "summary": respuesta_final,
            "handoff": True,
            "handoff_auto": True,  # â† Bloquea input
            "handoff_channel": channel,
            "handoff_department": department
        }
```

**Llamadas LLM en Nivel 4: 10 TOTAL**

---

## ğŸ¤– CONTEO DE LLAMADAS AL LLM

### Por Consulta Completa (Worst Case - Nivel 4)

| # | MÃ³dulo | FunciÃ³n | PropÃ³sito | CuÃ¡ndo |
|---|--------|---------|-----------|--------|
| 1 | intent_parser | interpretar_intencion_principal() | Extraer intenciÃ³n inicial | Stage ready |
| 2 | conversation_context | needs_context() | Detectar si necesita contexto | Si hay historial |
| 3 | conversation_context | enrich_query_with_context() | Enriquecer query con contexto | Si needs_context=true |
| 4 | intent_parser | interpretar_intencion_principal() | Re-interpretar query enriquecida | Si hubo enrichment |
| 5 | answerability | answerability_score() veredicto | Juez: Â¿se puede responder? | Por cada planned_query |
| 6 | answerability | gen_query_variants_llm() | Generar variantes de query | Si confidence < 0.65 |
| 7 | pdf_responder | responder_desde_pdfs() | Generar respuesta Nivel 1 | Si confidence â‰¥ 0.70 |
| 8 | pdf_responder | responder_desde_pdfs() | Generar respuesta Nivel 3 | Si 0.42 â‰¤ conf < 0.70 |
| 9 | handoff | classify_with_llm() | Clasificar departamento/canal | Nivel 4 (no info) |
| 10 | taxonomy | map_to_taxonomy() | Mapear a taxonomÃ­a | Nivel 4 |

**TOTAL WORST CASE: 10 llamadas al LLM**

### Por Consulta Optimizada (Best Case - Nivel 1)

| # | MÃ³dulo | FunciÃ³n | CuÃ¡ndo |
|---|--------|---------|--------|
| 1 | intent_parser | interpretar_intencion_principal() | Stage ready |
| 2 | answerability | answerability_score() veredicto | Nivel 1 |
| 3 | pdf_responder | responder_desde_pdfs() | Nivel 1 |

**TOTAL BEST CASE: 3 llamadas al LLM**

### Promedio Realista

- **Consulta simple con info clara**: ~3-5 llamadas
- **Consulta compleja sin info**: ~8-10 llamadas
- **Promedio general**: ~5-6 llamadas por consulta completa

---

## ğŸ” ITERACIONES Y LOOPS

### 1. Multi-Stage Retrieval (3 iteraciones mÃ¡x)

```python
for i, pq in enumerate(planned_queries[:3], 1):  # â† 3 ITERACIONES
    ascore = answerability_score(pq, retriever, k=12)  # LLM llamada por iteraciÃ³n
    
    if best_ascore is None or ascore["confidence"] > best_ascore["confidence"]:
        best_ascore = ascore
```

**Iteraciones: 3 mÃ¡ximo**
**LLM por iteraciÃ³n: 1 (veredicto)**

### 2. Query Variants Expansion (4 variantes)

```python
if ascore["confidence"] < 0.65:
    variants = gen_query_variants_llm(original_query, n=4)  # â† 1 LLM call genera 4 variantes
    
    for variant in variants:  # â† 4 ITERACIONES
        variant_score = answerability_score(variant, retriever)  # Sin LLM, solo metrics
```

**Iteraciones: 4 mÃ¡ximo**
**LLM: 1 para generar, 0 por variante (solo usa embeddings)**

### 3. Retrieval Interno (FAISS + BM25)

```python
# Dentro del retriever
docs = retriever.invoke(query)  # â† NO usa LLM, solo embeddings
# - Dense: MMR sobre FAISS (embeddings precalculados)
# - Sparse: BM25 (term frequency)
# - Ensemble: Combina ambos
```

**Iteraciones: 0 (bÃºsqueda vectorial)**
**LLM: 0**

### 4. RRF Fusion

```python
fused_docs = rrf_fuse([docs_q1, docs_q2, docs_q3])  # â† MatemÃ¡tica pura, no LLM
```

**Iteraciones: 1**
**LLM: 0**

---

## ğŸ¯ FEATURE FLAGS Y OPTIMIZACIONES

```python
# config.py
FEATURE_FLAGS = {
    "query_planner": True,          # âœ… Planner con subconsultas (aumenta recall)
    "rrf_fusion": True,             # âœ… RRF para combinar resultados
    "fuzzy_safety_net": False,      # âŒ OFF (usa RapidFuzz, lento)
    "entity_router": True,          # âœ… Router por entidades (EPUNEMI, SGA)
    "neutral_response": False,      # âŒ Respuesta sin "SegÃºn..."
    "cross_encoder_rerank": False,  # âŒ OFF (agrega 3-5 segundos)
}
```

### Optimizaciones Implementadas

1. **CachÃ© de Ãndices FAISS**: No reconstruye si PDFs no cambian
2. **CachÃ© de Retriever**: Reutiliza retriever entre consultas
3. **LÃ­mite de Queries**: MÃ¡ximo 3 planned queries (no 5)
4. **LÃ­mite de Variantes**: MÃ¡ximo 4 variantes LLM (no mÃ¡s)
5. **Cross-Encoder OFF**: Demasiado lento (3-5 seg), precision gain marginal
6. **Fuzzy Safety Net OFF**: RapidFuzz ralentiza sin beneficio claro

---

## ğŸ“Š TIEMPOS DE RESPUESTA

### Desglose por Componente (estimado)

| Componente | Tiempo | LLM Calls |
|------------|--------|-----------|
| InterpretaciÃ³n inicial | ~1-2s | 1 |
| Contexto conversacional | ~1-2s | 2 (si aplica) |
| Hierarchical routing | ~50-100ms | 0 |
| Query planning | ~10ms | 0 |
| Answerability (3 queries) | ~3-4s | 3 |
| Query variants LLM | ~2-3s | 1 |
| PDF respuesta | ~2-3s | 1 |
| Handoff classification | ~1-2s | 1 |
| Taxonomy mapping | ~1s | 1 |

**TOTAL WORST CASE: ~12-18 segundos**
**TOTAL BEST CASE: ~4-6 segundos**
**PROMEDIO: ~8-10 segundos**

### Bottlenecks Identificados

1. **LLM Calls**: ~1-2 segundos por llamada (quota rate limited)
2. **Embeddings FAISS**: ~100-300ms por query
3. **Multi-stage retrieval**: 3x las llamadas = 3x el tiempo

---

## ğŸ—„ï¸ BASE DE DATOS Y PERSISTENCIA

### Vectorstore (FAISS)

```python
# app/data/unemi_interno/estudiantes/
combined_index_<hash>/
â”œâ”€â”€ index.faiss         # Ãndice vectorial
â”œâ”€â”€ index.pkl           # Metadata
â””â”€â”€ .index_metadata_<hash>.json  # Info de PDFs incluidos
```

**56 PDFs** en total:
- 4 PDFs en `unemi_interno/estudiantes/`
- 2 PDFs en `unemi_interno/tic/`
- 1 PDF en `epunemi/`
- 49 PDFs en `legal_nacional/` (leyes, cÃ³digos, reglamentos)

### Ãndice FAISS

```python
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
)
# DimensiÃ³n: 384
# Chunks: 1500 caracteres, overlap 300
# Total documentos: ~200-300 chunks
```

### ChromaDB

```
chroma_db/
â””â”€â”€ chroma.sqlite3  # Base de datos SQLite de Chroma
```

**Nota**: Actualmente no se usa Chroma, solo FAISS.

### Django DB

```python
db.sqlite3  # Base de datos de Django (usuarios, sesiones)
```

---

## ğŸ¨ FRONTEND (Svelte)

### ChatbotInline.svelte (600 lÃ­neas)

**Responsabilidades:**

1. **UI del Chatbot**: Input, burbujas de mensajes, formularios
2. **State Management**: 
   - `messages`: Array de mensajes del chat
   - `conversationBlocked`: Flag de input bloqueado tras handoff
   - `currentCategory`, `currentSubcategory`: Contexto de la conversaciÃ³n
   - `studentData`: Datos del estudiante logueado
3. **ComunicaciÃ³n con Backend**:
   - `processMessage()`: EnvÃ­a mensaje al backend
   - `formatHistoryForBackend()`: Formatea historial para el API
4. **Manejo de Handoff AutomÃ¡tico**:
   - Detecta `handoff_auto: true` en respuesta
   - Bloquea input automÃ¡ticamente
   - Muestra mensaje de derivaciÃ³n

```javascript
// DetecciÃ³n de handoff automÃ¡tico
if (data.handoff_auto) {
  conversationBlocked = true;  // â† Bloquea input
}

// UI bloqueada
{#if conversationBlocked}
  <textarea disabled placeholder="ConversaciÃ³n derivada a agente..."></textarea>
  <button disabled>Derivado</button>
  <div class="blocked-notice">
    ğŸ“§ Un agente se pondrÃ¡ en contacto contigo por correo electrÃ³nico
  </div>
{/if}
```

---

## ğŸš€ DEPLOYMENT Y EJECUCIÃ“N

### Backend (Django)

```bash
# Activar entorno virtual
venv\Scripts\activate

# Instalar dependencias
pip install -r requirements.txt

# Crear/aplicar migraciones
python manage.py makemigrations
python manage.py migrate

# Iniciar servidor
python manage.py runserver
```

**Puerto**: http://localhost:8000

### Frontend (Svelte + Vite)

```bash
cd frontend

# Instalar dependencias
npm install

# Iniciar dev server
npm run dev
```

**Puerto**: http://localhost:5173

### Scripts de Inicio

```batch
:: iniciar-vite.bat
@echo off
cd frontend
npm run dev
```

```powershell
# iniciar-vite.ps1
Set-Location -Path "frontend"
npm run dev
```

---

## ğŸ” CONFIGURACIÃ“N

### Variables de Entorno

```python
# .env (en la raÃ­z del proyecto)
GOOGLE_API_KEY=<tu-api-key-de-gemini>
```

### ConfiguraciÃ³n LLM

```python
# app/services/config.py
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",  # Modelo experimental
    temperature=0,              # Determinista
    api_key=GOOGLE_API_KEY,
    max_retries=2,
)
```

**LÃ­mites de Quota (Free Tier)**:
- 10 requests/minuto
- Con 8-10 LLM calls por consulta â†’ ~1 consulta por minuto
- SoluciÃ³n: Implementar rate limiting o upgrade a paid tier

---

## ğŸ“ˆ MÃ‰TRICAS Y LOGGING

### Logs del Sistema

```python
print(f"ğŸ¯ Entidades detectadas: {entities}")
print(f"ğŸ“ˆ TÃ©rminos boosteados: {routing_info['boosts'][:3]}")
print(f"ğŸ“‚ Routing: {method} - {len(files)} files, {len(folders)} folders")
print(f"ğŸ” [{i}] '{query[:60]}...' â†’ conf: {ascore['confidence']:.3f}")
print(f"ğŸ“Š RESULTADO FINAL:")
print(f"   Confidence: {ascore.get('confidence', 0):.3f}")
print(f"   Docs recuperados: {ascore.get('non_empty_docs', 0)}")
print(f"   Verdict: {ascore.get('verdict', 'N/A')}")
```

### TelemetrÃ­a Implementada

- âœ… Confianza por query
- âœ… NÃºmero de documentos recuperados
- âœ… Veredicto del juez LLM
- âœ… MÃ©todo de routing usado
- âœ… Entidades detectadas
- âœ… Variantes generadas
- âœ… Decisiones de handoff

---

## ğŸ¯ RESUMEN EJECUTIVO

### Componentes Clave

1. **8 servicios backend** (3,163 lÃ­neas total)
2. **1 orquestador principal** (rag_chat_service.py - 691 lÃ­neas)
3. **10 llamadas LLM** (worst case) por consulta
4. **3 stages** de conversaciÃ³n (ready â†’ confirm â†’ RAG)
5. **4 niveles** de confianza (â‰¥0.70, 0.42-0.70, <0.42, crÃ­ticas)
6. **56 PDFs** en la base de conocimiento
7. **~200-300 chunks** indexados en FAISS
8. **3 iteraciones** multi-stage retrieval
9. **4 variantes** query expansion
10. **0 keywords** - todo evaluaciÃ³n LLM con JSON

### Flujo Completo Simplificado

```
Usuario escribe â†’ Frontend envÃ­a â†’
Backend interpreta (LLM #1) â†’ Pide confirmaciÃ³n â†’
Usuario confirma â†’ 
  â”œâ”€ Detecta contexto (LLM #2, #3, #4)
  â”œâ”€ Router jerÃ¡rquico (0 LLM)
  â”œâ”€ Multi-stage retrieval (LLM #5, #6)
  â”œâ”€ Genera respuesta (LLM #7, #8)
  â”œâ”€ Auto-evaluaciÃ³n (en JSON de #7, #8)
  â””â”€ Si no hay info â†’ Handoff (LLM #9, #10)
â†’ Frontend muestra respuesta
â†’ Si handoff_auto â†’ Bloquea input
```

### Performance

- â±ï¸ **4-18 segundos** por consulta (promedio 8-10s)
- ğŸ¤– **3-10 LLM calls** (promedio 5-6)
- ğŸ”„ **3 iteraciones** multi-stage mÃ¡ximo
- ğŸ“Š **Quota limit**: ~1 consulta/minuto (free tier)

### Optimizaciones Futuras

1. **CachÃ© de respuestas frecuentes** (Redis)
2. **Parallel LLM calls** (async donde sea posible)
3. **Reduce query variants** de 4 a 2
4. **Upgrade Gemini paid tier** para mÃ¡s quota
5. **Pre-compute embeddings** offline
6. **Streaming responses** para UX mÃ¡s rÃ¡pida

---


