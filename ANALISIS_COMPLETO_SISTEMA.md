# ğŸ“š ANÃLISIS COMPLETO DEL SISTEMA BALCONDEMO + PRIVATEGPT

## ğŸ—ï¸ ARQUITECTURA GENERAL

### Componentes Principales

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FRONTEND (Svelte)                        â”‚
â”‚  - BalconServicios.svelte                                   â”‚
â”‚  - ChatbotInline.svelte                                     â”‚
â”‚  - Formulario.svelte                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚ HTTP POST /api/chat/
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              BALCONDEMO (Django Backend)                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ views.py (chat_api)                                  â”‚   â”‚
â”‚  â”‚  - Recibe mensaje del usuario                        â”‚   â”‚
â”‚  â”‚  - Carga datos del estudiante (data_unemi.json)      â”‚   â”‚
â”‚  â”‚  - Llama a classify_with_privategpt()                â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                     â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ privategpt_chat_service.py                           â”‚   â”‚
â”‚  â”‚  - Maneja flujo de conversaciÃ³n                       â”‚   â”‚
â”‚  â”‚  - Interpreta intenciones                              â”‚   â”‚
â”‚  â”‚  - Maneja confirmaciones                              â”‚   â”‚
â”‚  â”‚  - Determina handoff                                  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                     â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ intent_parser.py                                      â”‚   â”‚
â”‚  â”‚  - Extrae intenciÃ³n del mensaje                       â”‚   â”‚
â”‚  â”‚  - Genera confirm_text                                â”‚   â”‚
â”‚  â”‚  - Determina answer_type                              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                     â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ privategpt_client.py                                  â”‚   â”‚
â”‚  â”‚  - Cliente HTTP para PrivateGPT                       â”‚   â”‚
â”‚  â”‚  - Construye mensajes con contexto de rol            â”‚   â”‚
â”‚  â”‚  - EnvÃ­a a /v1/chat/completions                       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚ HTTP POST
                       â”‚ /v1/chat/completions
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              PRIVATEGPT (FastAPI + LlamaIndex)              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ chat_router.py                                       â”‚   â”‚
â”‚  â”‚  - Recibe request                                    â”‚   â”‚
â”‚  â”‚  - Parsea JSON response                              â”‚   â”‚
â”‚  â”‚  - Procesa fuentes                                   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                     â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ chat_service.py                                      â”‚   â”‚
â”‚  â”‚  - Construye chat_engine                             â”‚   â”‚
â”‚  â”‚  - Combina system prompts                             â”‚   â”‚
â”‚  â”‚  - Expande consultas                                 â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                     â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ VectorStoreComponent                                  â”‚   â”‚
â”‚  â”‚  - Retriever con filtrado de archivos tmp            â”‚   â”‚
â”‚  â”‚  - BÃºsqueda semÃ¡ntica                                â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                     â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ LLM (Gemini)                                         â”‚   â”‚
â”‚  â”‚  - Genera respuesta con contexto                     â”‚   â”‚
â”‚  â”‚  - Retorna JSON con has_information                  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚ JSON Response
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              BALCONDEMO (Procesamiento)                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ privategpt_response_parser.py                        â”‚   â”‚
â”‚  â”‚  - Parsea respuesta JSON                             â”‚   â”‚
â”‚  â”‚  - Extrae has_information                            â”‚   â”‚
â”‚  â”‚  - Valida con heurÃ­sticas                           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                     â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ privategpt_chat_service.py                           â”‚   â”‚
â”‚  â”‚  - Agrupa fuentes                                    â”‚   â”‚
â”‚  â”‚  - Determina handoff si necesario                    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚ JSON Response
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FRONTEND                                 â”‚
â”‚  - Muestra respuesta                                        â”‚
â”‚  - Muestra fuentes                                          â”‚
â”‚  - Maneja confirmaciones                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ FLUJO COMPLETO DE UNA CONSULTA

### Ejemplo: "necesito saber como cambiar de carrera"

#### **PASO 1: Frontend â†’ Django (views.py)**

```python
# Frontend envÃ­a POST a /api/chat/
{
  "message": "necesito saber como cambiar de carrera",
  "conversation_history": [],
  "usuario_cedula": "1234567890",
  "perfil_id": "1"
}
```

**Archivo**: `views.py` â†’ `chat_api()`
- Recibe el request
- Carga `student_data` desde `data_unemi.json`
- Llama a `classify_with_privategpt()`

---

#### **PASO 2: DetecciÃ³n de Stage y Saludo**

**Archivo**: `privategpt_chat_service.py` â†’ `classify_with_privategpt()`

```python
# Detecta stage desde historial
stage = _detect_stage_from_history(conversation_history)
# Resultado: ConversationStage.AWAIT_INTENT

# Si es saludo â†’ respuesta directa
if es_greeting(user_text):
    return {"summary": "Hola! ğŸ‘‹ Soy tu asistente..."}
```

---

#### **PASO 3: InterpretaciÃ³n de IntenciÃ³n (1Âª LLM Call)**

**Archivo**: `intent_parser.py` â†’ `interpretar_intencion_principal()`

**Prompt enviado al LLM**:
```
INTENT_SYSTEM:
Eres un extractor de intenciÃ³n. Devuelve SOLO un JSON vÃ¡lido con esta estructura mÃ­nima:

{
  "intent_short": "<12-16 palabras, concreta y accionable>",
  "intent_code": "<uno de: consultar_solicitudes_balcon | consultar_datos_personales | consultar_carrera_actual | consultar_roles_usuario | otro>",
  "accion": "<verbo principal en infinitivo: consultar, rectificar, recalificar, cambiar, inscribir, homologar, pagar, solicitar, etc.>",
  "objeto": "<quÃ© cosa sobre la que recae la acciÃ³n: nota, actividad, paralelo, carrera, matrÃ­cula, prÃ¡ctica, beca, certificado, etc.>",
  ...
  "needs_confirmation": <true o false>,
  "confirm_text": "<texto corto de confirmaciÃ³n en espaÃ±ol, listo para mostrar al usuario>",
  "answer_type": "<informativo o operativo>"
}

TEXTO:
necesito saber como cambiar de carrera
```

**Respuesta del LLM**:
```json
{
  "intent_short": "consultar informaciÃ³n sobre cambio de carrera",
  "intent_code": "otro",
  "accion": "consultar",
  "objeto": "carrera",
  "detalle_libre": "necesito saber como cambiar de carrera",
  "needs_confirmation": false,
  "confirm_text": "Â¿Quieres consultar informaciÃ³n sobre cÃ³mo cambiar de carrera?",
  "answer_type": "informativo"
}
```

**NormalizaciÃ³n**:
- `needs_confirmation`: `"false"` â†’ `False` (bool)
- `answer_type`: `"informativo"` â†’ `"informativo"` (validado)

---

#### **PASO 4: VerificaciÃ³n de ConfirmaciÃ³n**

**Archivo**: `privategpt_chat_service.py` â†’ `classify_with_privategpt()`

```python
needs_confirmation = intent_slots.get("needs_confirmation", True)

if not needs_confirmation:
    # Proceder directamente sin mostrar confirmaciÃ³n
    return _handle_confirmation_stage(...)
```

**Como `needs_confirmation = False`**, se procede directamente a la siguiente etapa.

---

#### **PASO 5: ClasificaciÃ³n HeurÃ­stica (Sin LLM)**

**Archivo**: `handoff.py` â†’ `classify_with_heuristics()`

```python
# Determina department y channel usando reglas heurÃ­sticas
intent_code = "otro"
accion = "consultar"
objeto = "carrera"

# Reglas heurÃ­sticas:
if "carrera" in objeto:
    # Buscar en handoff_config.json
    department = "acadÃ©mico"
    channel = "DIRECCIÃ“N DE GESTIÃ“N Y SERVICIOS ACADÃ‰MICOS"
```

**Resultado**:
```python
{
    "answer_type": "informativo",
    "department": "acadÃ©mico",
    "channel": "DIRECCIÃ“N DE GESTIÃ“N Y SERVICIOS ACADÃ‰MICOS",
    "reasoning": "Clasificado por reglas heurÃ­sticas"
}
```

---

#### **PASO 6: BÃºsqueda de Solicitudes Relacionadas**

**Archivo**: `related_request_matcher.py` â†’ `find_related_requests()`

- Busca solicitudes previas del estudiante relacionadas con "cambio de carrera"
- Si encuentra â†’ muestra opciÃ³n para relacionar
- Si no encuentra â†’ continÃºa con PrivateGPT

**En este caso**: No hay solicitudes relacionadas â†’ continÃºa.

---

#### **PASO 7: ConstrucciÃ³n del Mensaje para PrivateGPT**

**Archivo**: `privategpt_chat_service.py` â†’ `_build_role_context_message()`

**ExtracciÃ³n de rol**:
```python
rol = _extract_user_role(student_data, perfil_id)
# Resultado: "estudiante"
```

**Mensajes construidos**:
```python
messages = [
    {
        "role": "system",
        "content": """ROL DEL USUARIO: ESTUDIANTE

FILTRADO CRITICO:
- SOLO usa documentos para ESTUDIANTES (reglamentos estudiantiles, procesos acadÃ©micos estudiantiles, servicios estudiantiles, becas estudiantiles)
- IGNORA documentos para PROFESORES (reglamento docente, escalafÃ³n docente, evaluaciÃ³n docente)
- IGNORA documentos para PERSONAL ADMINISTRATIVO
- Si el contexto contiene SOLO informaciÃ³n para profesores/administrativos, establece has_information=false"""
    },
    {
        "role": "user",
        "content": "necesito saber como cambiar de carrera"
    }
]
```

**Session Context**:
```python
session_context = {
    "user_role": "estudiante",
    "profile_type": "ESTUDIANTE",
    "carrera": "IngenierÃ­a de Software",
    "facultad": "Facultad de Ciencias e IngenierÃ­a"
}
```

---

#### **PASO 8: EnvÃ­o a PrivateGPT**

**Archivo**: `privategpt_client.py` â†’ `chat_completion()`

**Request HTTP POST**:
```http
POST http://localhost:8001/v1/chat/completions
Content-Type: application/json

{
  "messages": [
    {"role": "system", "content": "ROL DEL USUARIO: ESTUDIANTE\n\nFILTRADO CRITICO:..."},
    {"role": "user", "content": "necesito saber como cambiar de carrera"}
  ],
  "use_context": true,
  "include_sources": true,
  "stream": false,
  "session_context": {
    "user_role": "estudiante",
    ...
  }
}
```

---

#### **PASO 9: Procesamiento en PrivateGPT**

**Archivo**: `chat_service.py` â†’ `_chat_engine()`

**1. CombinaciÃ³n de System Prompts**:

PrivateGPT combina automÃ¡ticamente:
- `default_query_system_prompt` (de `settings-docker.yaml`)
- System message personalizado (de BalconDemo)

**Prompt final combinado**:
```
Eres un asistente RAG. Debes responder exclusivamente con un JSON vÃ¡lido en UTF-8,
sin texto adicional, sin backticks y sin bloques de cÃ³digo.

Formato de salida obligatorio:
- Un objeto JSON con las claves:
  - has_information: booleano (true o false)
  - response: string en espaÃ±ol, claro y natural
  - fuentes: lista de objetos; cada objeto con la clave pagina (string)

---

ROL DEL USUARIO: ESTUDIANTE

FILTRADO CRITICO:
- SOLO usa documentos para ESTUDIANTES (reglamentos estudiantiles, procesos acadÃ©micos estudiantiles, servicios estudiantiles, becas estudiantiles)
- IGNORA documentos para PROFESORES (reglamento docente, escalafÃ³n docente, evaluaciÃ³n docente)
- IGNORA documentos para PERSONAL ADMINISTRATIVO
- Si el contexto contiene SOLO informaciÃ³n para profesores/administrativos, establece has_information=false
```

**2. ExpansiÃ³n de Consulta** (si estÃ¡ habilitada):

**Archivo**: `chat_service.py` â†’ `_expand_query()`

**Prompt para expansiÃ³n**:
```
Dada la siguiente consulta del usuario, genera 2-3 variaciones o reformulaciones que puedan ayudar a encontrar informaciÃ³n relacionada en documentos.

Consulta original: necesito saber como cambiar de carrera

Genera solo las variaciones, una por lÃ­nea, sin numeraciÃ³n ni explicaciones:
```

**Respuesta del LLM**:
```
cÃ³mo hacer una transiciÃ³n profesional
guÃ­a para reorientaciÃ³n laboral
pasos para cambiar de profesiÃ³n
```

**3. BÃºsqueda en Vector Store**:

**Archivo**: `vector_store_component.py` â†’ `get_retriever()`

- Busca con la consulta original
- Busca con cada variaciÃ³n expandida
- **Filtra archivos temporales** (que empiezan con "tmp")
- Combina resultados Ãºnicos
- Ordena por score (relevancia)

**4. Filtrado por Rol**:

**Archivo**: `role_based_postprocessor.py` (si user_role = "estudiante")

- Prioriza documentos que empiezan con `unemi_`
- Reordena resultados para estudiantes

**5. Reranking** (opcional, actualmente deshabilitado):

- Usa modelo cross-encoder para reordenar por relevancia

**6. ConstrucciÃ³n del Contexto**:

LlamaIndex construye el contexto final con:
- Top 10 documentos mÃ¡s relevantes (similarity_top_k)
- Chunks de texto de cada documento
- Metadata (file_name, page_label)

**Contexto enviado al LLM**:
```
Use the context information below to assist the user.
--------------------
[Chunk 1 del documento mÃ¡s relevante]
[Chunk 2 del documento mÃ¡s relevante]
...
--------------------
ROL DEL USUARIO: ESTUDIANTE

FILTRADO CRITICO:
- SOLO usa documentos para ESTUDIANTES...
```

---

#### **PASO 10: GeneraciÃ³n de Respuesta (2Âª LLM Call)**

**Archivo**: `chat_service.py` â†’ `chat()`

**Prompt completo al LLM**:
```
[System prompt combinado con instrucciones JSON + filtrado por rol]

[Contexto de documentos recuperados]

[User message: "necesito saber como cambiar de carrera"]
```

**Respuesta del LLM**:
```json
{
  "has_information": true,
  "response": "Para cambiar de carrera en la UNEMI, debes seguir los siguientes pasos:\n\n1. Presentar una solicitud formal al departamento acadÃ©mico correspondiente.\n2. Cumplir con los requisitos acadÃ©micos establecidos.\n3. Obtener la aprobaciÃ³n del consejo acadÃ©mico.\n\nLos detalles especÃ­ficos se encuentran en el Reglamento de Carreras.",
  "fuentes": [
    {"pagina": "15"},
    {"pagina": "16"}
  ]
}
```

**O si no encuentra informaciÃ³n**:
```
has_information: false
Lo siento, la informaciÃ³n proporcionada en el contexto no contiene detalles sobre cÃ³mo cambiar de carrera para estudiantes...
```

---

#### **PASO 11: Parseo de Respuesta en PrivateGPT**

**Archivo**: `chat_router.py` â†’ `parse_json_response()`

**Proceso**:
1. Intenta parsear como JSON completo
2. Si falla, busca JSON parcial (`{"has_information": ...}`)
3. Si falla, busca patrÃ³n `has_information: false` en texto plano
4. Si falla, usa heurÃ­sticas (frases negativas, longitud)

**Resultado parseado**:
```python
{
    "has_information": True,
    "response": "Para cambiar de carrera...",
    "fuentes": [{"pagina": "15"}, {"pagina": "16"}]
}
```

**ReconstrucciÃ³n del JSON**:
```python
# SIEMPRE reconstruye JSON con has_information (True o False)
response_json = json.dumps({
    "has_information": True,
    "response": "Para cambiar de carrera...",
    "fuentes": sources_list  # Con metadata completo
})
```

---

#### **PASO 12: Respuesta HTTP a BalconDemo**

**Response de PrivateGPT**:
```json
{
  "id": "uuid",
  "object": "completion",
  "created": 1234567890,
  "model": "private-gpt",
  "choices": [{
    "message": {
      "role": "assistant",
      "content": "{\"has_information\":true,\"response\":\"Para cambiar de carrera...\",\"fuentes\":[{\"pagina\":\"15\"}]}"
    },
    "sources": [
      {
        "doc_id": "...",
        "score": 0.85,
        "document": {
          "doc_metadata": {
            "file_name": "Reglamento-Carreras.pdf",
            "page_label": "15"
          }
        }
      }
    ]
  }]
}
```

---

#### **PASO 13: Parseo en BalconDemo**

**Archivo**: `privategpt_response_parser.py` â†’ `parse()`

**Proceso**:
1. Extrae `content` del message
2. Intenta parsear como JSON
3. Si falla, busca patrÃ³n `has_information: false`
4. Extrae `response` y `has_information`
5. Extrae `fuentes` desde `sources` del response

**Resultado**:
```python
{
    "has_information": True,
    "response": "Para cambiar de carrera...",
    "fuentes": [
        {"archivo": "Reglamento-Carreras.pdf", "pagina": "15"},
        {"archivo": "Reglamento-Carreras.pdf", "pagina": "16"}
    ]
}
```

**ValidaciÃ³n con HeurÃ­sticas**:
```python
if has_information_from_json is not None:
    # Validar que si viene true, realmente haya informaciÃ³n Ãºtil
    has_information = _validate_has_information(
        has_information_from_json, response_text, fuentes
    )
```

**HeurÃ­sticas de validaciÃ³n**:
- Si `has_information = False` â†’ devolver `False`
- Si `has_information = True` pero:
  - No hay fuentes â†’ `False`
  - Respuesta muy corta (< 50 chars) â†’ `False`
  - Contiene patrones de disculpa â†’ `False`
- Si pasa todas las validaciones â†’ `True`

---

#### **PASO 14: AgrupaciÃ³n de Fuentes**

**Archivo**: `privategpt_chat_service.py` â†’ `_agrupar_fuentes_por_archivo()`

**Entrada**:
```python
[
    {"archivo": "Reglamento-Carreras.pdf", "pagina": "15"},
    {"archivo": "Reglamento-Carreras.pdf", "pagina": "16"},
    {"archivo": "Reglamento-Carreras.pdf", "pagina": "15"}  # Duplicado
]
```

**Salida**:
```python
[
    {
        "archivo": "Reglamento-Carreras.pdf",
        "paginas": ["15", "16"]  # Ordenadas y sin duplicados
    }
]
```

---

#### **PASO 15: ConstrucciÃ³n de Respuesta Final**

**Archivo**: `privategpt_chat_service.py` â†’ `_handle_confirmation_stage()`

**Si `has_information = True`**:
```python
return {
    "summary": "Para cambiar de carrera...",
    "has_information": True,
    "fuentes": [
        {"archivo": "Reglamento-Carreras.pdf", "paginas": ["15", "16"]}
    ],
    "source_pdfs": ["Reglamento-Carreras.pdf"],
    "needs_confirmation": False,
    "confirmed": True,
    ...
}
```

**Si `has_information = False`**:
```python
# Determinar departamento para handoff
depto = _determinar_departamento_handoff(...)
# Resultado: "DIRECCIÃ“N DE GESTIÃ“N Y SERVICIOS ACADÃ‰MICOS"

return {
    "summary": "Este caso necesita ser revisado por mis compaÃ±eros humanos...",
    "has_information": False,
    "handoff": True,
    "handoff_channel": depto,
    "needs_handoff_details": True,
    ...
}
```

---

#### **PASO 16: Respuesta al Frontend**

**Archivo**: `views.py` â†’ `chat_api()`

**JSON Response**:
```json
{
  "message": "Para cambiar de carrera...",
  "response": "Para cambiar de carrera...",
  "has_information": true,
  "fuentes": [
    {
      "archivo": "Reglamento-Carreras.pdf",
      "paginas": ["15", "16"]
    }
  ],
  "source_pdfs": ["Reglamento-Carreras.pdf"],
  "needs_confirmation": false,
  "confirmed": true,
  "intent_slots": {
    "intent_short": "consultar informaciÃ³n sobre cambio de carrera",
    "answer_type": "informativo",
    ...
  },
  ...
}
```

---

## ğŸ“ SISTEMA DE PROMPTS

### 1. Prompt de IntenciÃ³n (BalconDemo â†’ LLM)

**Archivo**: `intent_parser.py` â†’ `INTENT_SYSTEM`

**PropÃ³sito**: Extraer intenciÃ³n estructurada del mensaje del usuario

**Estructura del JSON esperado**:
```json
{
  "intent_short": "string",
  "intent_code": "string",
  "accion": "string",
  "objeto": "string",
  "needs_confirmation": boolean,
  "confirm_text": "string",
  "answer_type": "informativo" | "operativo"
}
```

**Reglas clave**:
- `needs_confirmation`: `true` si la intenciÃ³n no estÃ¡ 100% clara
- `answer_type`: 
  - `"informativo"`: preguntas, consultas, "cÃ³mo hacer X"
  - `"operativo"`: cambios de estado, trÃ¡mites que requieren acciÃ³n humana
- `confirm_text`: frase amigable en espaÃ±ol para confirmar

---

### 2. Prompt del Sistema en PrivateGPT

**Archivo**: `settings-docker.yaml` â†’ `default_query_system_prompt`

**PropÃ³sito**: Instruir al LLM a retornar JSON con formato especÃ­fico

**Contenido**:
```
Eres un asistente RAG. Debes responder exclusivamente con un JSON vÃ¡lido en UTF-8,
sin texto adicional, sin backticks y sin bloques de cÃ³digo.

Formato de salida obligatorio:
- Un objeto JSON con las claves:
  - has_information: booleano (true o false)
  - response: string en espaÃ±ol, claro y natural
  - fuentes: lista de objetos; cada objeto con la clave pagina (string)
```

**CombinaciÃ³n con prompt personalizado**:
- PrivateGPT combina automÃ¡ticamente `default_query_system_prompt` + system message de BalconDemo
- El system message de BalconDemo contiene solo instrucciones de filtrado por rol
- El formato JSON viene del `default_query_system_prompt`

---

### 3. Prompt de Filtrado por Rol

**Archivo**: `privategpt_chat_service.py` â†’ `_build_role_context_message()`

**Para ESTUDIANTE**:
```
ROL DEL USUARIO: ESTUDIANTE

FILTRADO CRITICO:
- SOLO usa documentos para ESTUDIANTES (reglamentos estudiantiles, procesos acadÃ©micos estudiantiles, servicios estudiantiles, becas estudiantiles)
- IGNORA documentos para PROFESORES (reglamento docente, escalafÃ³n docente, evaluaciÃ³n docente)
- IGNORA documentos para PERSONAL ADMINISTRATIVO
- Si el contexto contiene SOLO informaciÃ³n para profesores/administrativos, establece has_information=false
```

**Para PROFESOR**:
```
ROL DEL USUARIO: PROFESOR

FILTRADO CRITICO:
- SOLO usa documentos para PROFESORES (reglamento docente, escalafÃ³n docente, evaluaciÃ³n docente, procesos acadÃ©micos para profesores)
- IGNORA documentos para ESTUDIANTES (procesos de matrÃ­cula estudiantil, servicios estudiantiles)
- IGNORA documentos para PERSONAL ADMINISTRATIVO
- Si el contexto contiene SOLO informaciÃ³n para estudiantes/administrativos, establece has_information=false
```

**Para EXTERNO**:
```
ROL DEL USUARIO: EXTERNO

FILTRADO:
- Prioriza informaciÃ³n general de la universidad
- Evita informaciÃ³n muy especÃ­fica de procesos internos
- Si el contexto contiene informaciÃ³n muy especÃ­fica de procesos internos, establece has_information=false
```

---

## ğŸ” SISTEMA DE DETECCIÃ“N DE `has_information`

### Flujo Completo

#### **1. GeneraciÃ³n en PrivateGPT (LLM)**

El LLM genera la respuesta y decide `has_information` basÃ¡ndose en:
- Si encontrÃ³ informaciÃ³n relevante en el contexto
- Si el contexto contiene solo informaciÃ³n para otros roles
- Si la respuesta es una disculpa

**Formato esperado**:
```json
{
  "has_information": true,
  "response": "...",
  "fuentes": [...]
}
```

**O**:
```
has_information: false
Lo siento, no encontrÃ© informaciÃ³n...
```

---

#### **2. Parseo en PrivateGPT (`chat_router.py`)**

**FunciÃ³n**: `parse_json_response()`

**Proceso**:
1. **Intenta parsear JSON completo**: Busca `{...}` balanceado
2. **Intenta parsear JSON parcial**: Busca `{"has_information": ...}` embebido
3. **Busca patrÃ³n en texto plano**: `has_information: false` o `has_information=false`
4. **Fallback heurÃ­stico**: 
   - Busca frases negativas ("no tengo informaciÃ³n", "lo siento", etc.)
   - Verifica longitud mÃ­nima (< 50 chars â†’ `false`)
   - Si no hay fuentes y es disculpa â†’ `false`

**Patrones detectados**:
- `has_information: false`
- `has_information=false`
- `has_information = false`
- `"has_information": false`

**ReconstrucciÃ³n del JSON**:
```python
# SIEMPRE reconstruye JSON con has_information (True o False)
response_json = json.dumps({
    "has_information": has_info,  # True o False
    "response": clean_response,
    "fuentes": sources_list
})
```

---

#### **3. Parseo en BalconDemo (`privategpt_response_parser.py`)**

**FunciÃ³n**: `_extract_response_and_has_info()`

**Proceso similar**:
1. Intenta parsear JSON completo
2. Intenta parsear JSON parcial
3. Busca patrÃ³n `has_information: false` en texto plano
4. Si no encuentra â†’ `has_information = None`

**ValidaciÃ³n con HeurÃ­sticas**:
```python
if has_information_from_json is not None:
    # Validar que si viene true, realmente haya informaciÃ³n Ãºtil
    has_information = _validate_has_information(
        has_information_from_json, response_text, fuentes
    )
```

**HeurÃ­sticas de `_validate_has_information()`**:
- Si `has_information = False` â†’ devolver `False`
- Si `has_information = True` pero:
  - No hay fuentes â†’ `False`
  - Respuesta muy corta (< 50 chars) â†’ `False`
  - Contiene patrones de disculpa â†’ `False`
- Si pasa todas â†’ `True`

**Patrones de disculpa** (`DISCULPA_PATTERNS`):
```python
[
    "no tengo informaciÃ³n",
    "no encuentro informaciÃ³n",
    "no puedo ayudarte",
    "te sugiero que te pongas en contacto",
    "lo siento, no",
    "lamentablemente no encontrÃ©",
    ...
]
```

---

## ğŸ¯ SISTEMA DE INTENCIONES

### ExtracciÃ³n de IntenciÃ³n

**Archivo**: `intent_parser.py` â†’ `interpretar_intencion_principal()`

**LLM Call**: 1 llamada al LLM (Gemini)

**Input**:
```
INTENT_SYSTEM + "\n\nTEXTO:\n" + user_text
```

**Output JSON**:
```json
{
  "intent_short": "consultar informaciÃ³n sobre cambio de carrera",
  "intent_code": "otro",
  "accion": "consultar",
  "objeto": "carrera",
  "asignatura": "",
  "unidad_o_actividad": "",
  "periodo": "",
  "detalle_libre": "necesito saber como cambiar de carrera",
  "original_user_message": "necesito saber como cambiar de carrera",
  "needs_confirmation": false,
  "confirm_text": "Â¿Quieres consultar informaciÃ³n sobre cÃ³mo cambiar de carrera?",
  "answer_type": "informativo"
}
```

**NormalizaciÃ³n**:
- `needs_confirmation`: Convierte string a bool
- `answer_type`: Valida que sea "informativo" o "operativo"
- Si `confirm_text` estÃ¡ vacÃ­o pero `needs_confirmation = True` â†’ genera uno bÃ¡sico

---

### ClasificaciÃ³n HeurÃ­stica (Sin LLM)

**Archivo**: `handoff.py` â†’ `classify_with_heuristics()`

**Input**: `intent_slots` (del paso anterior)

**Proceso**:
1. Extrae `intent_code`, `accion`, `objeto`, `detalle_libre`
2. Aplica reglas heurÃ­sticas basadas en `handoff_config.json`
3. Determina `answer_type`, `department`, `channel`

**Reglas de ejemplo**:
```python
# Cambio de paralelo
if "paralelo" in texto or "paralelo" in objeto:
    department = "acadÃ©mico"
    channel = "DIRECCIÃ“N DE GESTIÃ“N Y SERVICIOS ACADÃ‰MICOS"

# Beca estudiantil
elif "beca" in texto or "beca" in objeto:
    department = "bienestar"
    channel = "DIRECCIÃ“N DE BIENESTAR UNIVERSITARIO"

# Biblioteca
elif "biblioteca" in texto or "libro" in texto:
    department = "general"
    channel = "CENTRO DE RECURSOS PARA EL APRENDIZAJE Y LA INVESTIGACIÃ“N"
```

**Output**:
```python
{
    "answer_type": "informativo",
    "department": "acadÃ©mico",
    "channel": "DIRECCIÃ“N DE GESTIÃ“N Y SERVICIOS ACADÃ‰MICOS",
    "reasoning": "Clasificado por reglas heurÃ­sticas"
}
```

---

## ğŸ”€ SISTEMA DE HANDOFF

### DeterminaciÃ³n de Departamento

**Archivo**: `privategpt_chat_service.py` â†’ `_determinar_departamento_handoff()`

**Prioridad**:
1. **Desde categorÃ­a/subcategorÃ­a** (si estÃ¡n disponibles):
   ```python
   if category and subcategory:
       depto = get_departamento_real(category, subcategory)
   ```

2. **Desde heurÃ­sticas** (si hay `intent_slots`):
   ```python
   if intent_slots:
       heuristic_classification = classify_with_heuristics(intent_slots)
       depto = heuristic_classification.get("channel")
   ```

3. **Por defecto**:
   ```python
   default_depto = "DIRECCIÃ“N DE GESTIÃ“N Y SERVICIOS ACADÃ‰MICOS"
   ```

---

### Mapeo de CategorÃ­as

**Archivo**: `handoff_config.json`

**Estructura**:
```json
{
  "mapeo_categoria_subcategoria": {
    "Academico": {
      "Cambio de paralelo": "DIRECCIÃ“N DE GESTIÃ“N Y SERVICIOS ACADÃ‰MICOS",
      "Cambio de carrera": "DIRECCIÃ“N DE GESTIÃ“N Y SERVICIOS ACADÃ‰MICOS",
      ...
    },
    "Bienestar estudiantil": {
      "Beca estudiantil": "DIRECCIÃ“N DE BIENESTAR UNIVERSITARIO",
      ...
    }
  },
  "mapeo_por_intencion": {
    "beca": "DIRECCIÃ“N DE BIENESTAR UNIVERSITARIO",
    "biblioteca": "CENTRO DE RECURSOS PARA EL APRENDIZAJE Y LA INVESTIGACIÃ“N",
    ...
  }
}
```

---

## ğŸ”„ FLUJO DE ESTADOS (Stages)

### Estados Posibles

```python
class ConversationStage(Enum):
    GREETING = "greeting"                    # Saludo inicial
    AWAIT_INTENT = "await_intent"            # Esperando interpretar intenciÃ³n
    AWAIT_CONFIRM = "await_confirm"          # Esperando confirmaciÃ³n del usuario
    AWAIT_RELATED_REQUEST = "await_related_request"  # Esperando selecciÃ³n de solicitud relacionada
    AWAIT_HANDOFF_DETAILS = "await_handoff_details"  # Esperando detalles para handoff
```

### DetecciÃ³n de Stage

**Archivo**: `privategpt_chat_service.py` â†’ `_detect_stage_from_history()`

**LÃ³gica**:
1. Busca en el historial el Ãºltimo mensaje con `needs_confirmation: true`
2. Si encuentra â†’ `AWAIT_CONFIRM`
3. Busca mensaje con `needs_related_request_selection: true`
4. Si encuentra â†’ `AWAIT_RELATED_REQUEST`
5. Busca mensaje con `handoff: true` y `needs_handoff_details: true`
6. Si encuentra â†’ `AWAIT_HANDOFF_DETAILS`
7. Por defecto â†’ `AWAIT_INTENT`

---

## ğŸ“Š CONTEO DE LLAMADAS AL LLM

### Por Consulta Informativa Normal

1. **`interpretar_intencion_principal()`** â†’ **1 LLM call**
   - Extrae intenciÃ³n, `needs_confirmation`, `confirm_text`, `answer_type`

2. **PrivateGPT RAG** â†’ **1 LLM call**
   - ExpansiÃ³n de consulta (opcional, si estÃ¡ habilitada) â†’ **+1 LLM call**
   - GeneraciÃ³n de respuesta con contexto â†’ **1 LLM call**

**Total**: **2-3 LLM calls** (dependiendo de si la expansiÃ³n estÃ¡ habilitada)

---

### Por Consulta Operativa

1. **`interpretar_intencion_principal()`** â†’ **1 LLM call**

**Total**: **1 LLM call**

---

### Por ReinterpretaciÃ³n

Si el usuario dice "no, eso no era":
- Otra vez `interpretar_intencion_principal()` â†’ **+1 LLM call**
- (Opcionalmente) PrivateGPT RAG â†’ **+1 LLM call**

**Total adicional**: **1-2 LLM calls**

---

## ğŸ§¹ SISTEMA DE LIMPIEZA DE ARCHIVOS TEMPORALES

### Limpieza AutomÃ¡tica

**1. Al iniciar PrivateGPT** (`ingest_service.py`):
```python
def __init__(self, ...):
    ...
    self._cleanup_tmp_files_on_startup()
```

**2. DespuÃ©s de cada ingestiÃ³n** (`privategpt_client.py`):
```python
def ingest_file(self, file_path: str):
    result = response.json()
    self._cleanup_tmp_files()  # AutomÃ¡tico
    return result
```

**3. Al iniciar Django** (`apps.py`):
```python
def ready(self):
    client.cleanup_all_tmp_files()
```

---

### Filtrado en BÃºsquedas RAG

**Archivo**: `vector_store_component.py` â†’ `get_retriever()`

**Wrapper `FilteredRetriever`**:
```python
def retrieve(self, query_bundle):
    nodes = self.base_retriever.retrieve(query_bundle)
    # Filtrar nodos de archivos temporales
    filtered_nodes = [
        node for node in nodes
        if not node_metadata.get('file_name', '').lower().startswith('tmp')
    ]
    return filtered_nodes
```

**TambiÃ©n en expansiÃ³n de consultas** (`chat_service.py`):
```python
# Filtrar archivos que empiezan con "tmp"
if file_name.lower().startswith('tmp'):
    continue
```

---

## ğŸ” SISTEMA DE ROLES Y FILTRADO

### ExtracciÃ³n de Rol

**Archivo**: `privategpt_chat_service.py` â†’ `_extract_user_role()`

**Fuente**: `student_data` desde `data_unemi.json`

**Estructura**:
```json
{
  "perfiles": [
    {
      "id": "1",
      "es_estudiante": true,
      "es_profesor": false,
      "status": true,
      "inscripcionprincipal": true,
      ...
    }
  ]
}
```

**LÃ³gica**:
1. Busca perfil por `perfil_id` (si se proporciona)
2. Si no, busca perfil con `inscripcionprincipal: true`
3. Si no, usa el primer perfil activo
4. Determina rol segÃºn flags: `es_estudiante`, `es_profesor`, etc.

---

### Filtrado por Rol en PrivateGPT

**Archivo**: `role_based_postprocessor.py`

**Para estudiantes**:
- Prioriza documentos que empiezan con `unemi_`
- Reordena resultados para que documentos estudiantiles aparezcan primero

**Archivo**: `chat_service.py` â†’ `_chat_engine()`

```python
if user_role == "estudiante":
    role_postprocessor = RoleBasedPostprocessor(user_role=user_role)
    node_postprocessors.append(role_postprocessor)
```

---

## ğŸ“¡ COMUNICACIÃ“N HTTP

### Request de BalconDemo a PrivateGPT

**Endpoint**: `POST http://localhost:8001/v1/chat/completions`

**Headers**:
```http
Content-Type: application/json
Connection: close
```

**Body**:
```json
{
  "messages": [
    {"role": "system", "content": "ROL DEL USUARIO: ESTUDIANTE\n\nFILTRADO CRITICO:..."},
    {"role": "user", "content": "necesito saber como cambiar de carrera"}
  ],
  "use_context": true,
  "include_sources": true,
  "stream": false,
  "session_context": {
    "user_role": "estudiante",
    "profile_type": "ESTUDIANTE",
    "carrera": "IngenierÃ­a de Software",
    "facultad": "Facultad de Ciencias e IngenierÃ­a"
  }
}
```

---

### Response de PrivateGPT a BalconDemo

**Status**: `200 OK`

**Body**:
```json
{
  "id": "uuid",
  "object": "completion",
  "created": 1234567890,
  "model": "private-gpt",
  "choices": [{
    "message": {
      "role": "assistant",
      "content": "{\"has_information\":true,\"response\":\"Para cambiar de carrera...\",\"fuentes\":[{\"pagina\":\"15\"}]}"
    },
    "sources": [
      {
        "doc_id": "...",
        "score": 0.85,
        "document": {
          "doc_metadata": {
            "file_name": "Reglamento-Carreras.pdf",
            "page_label": "15"
          }
        }
      }
    ]
  }]
}
```

---

## ğŸ¨ FLUJO DE CONFIRMACIONES

### Cuando `needs_confirmation = True`

**Flujo**:
1. Usuario envÃ­a mensaje
2. `interpretar_intencion_principal()` retorna `needs_confirmation: true`
3. Sistema muestra `confirm_text` al usuario
4. Usuario confirma ("sÃ­", "correcto", etc.)
5. Sistema procede con la intenciÃ³n confirmada

**Ejemplo**:
```
Usuario: "quiero cambiar"
LLM: needs_confirmation=true, confirm_text="Â¿Quieres solicitar un cambio de paralelo?"
Sistema muestra: "Â¿Quieres solicitar un cambio de paralelo?"
Usuario: "sÃ­"
Sistema procede con cambio de paralelo
```

---

### Cuando `needs_confirmation = False`

**Flujo**:
1. Usuario envÃ­a mensaje
2. `interpretar_intencion_principal()` retorna `needs_confirmation: false`
3. Sistema procede directamente sin mostrar confirmaciÃ³n

**Ejemplo**:
```
Usuario: "necesito saber como cambiar de carrera"
LLM: needs_confirmation=false, answer_type="informativo"
Sistema procede directamente a PrivateGPT
```

---

## ğŸ”„ FLUJO DE SOLICITUDES RELACIONADAS

### BÃºsqueda de Solicitudes Relacionadas

**Archivo**: `related_request_matcher.py` â†’ `find_related_requests()`

**Proceso**:
1. Carga solicitudes previas del estudiante
2. Compara con la intenciÃ³n actual usando embeddings o texto
3. Retorna solicitudes relacionadas (mÃ¡ximo 3)

**Si encuentra solicitudes relacionadas**:
- Muestra opciÃ³n para relacionar
- Usuario puede seleccionar una o continuar sin relacionar

**Si no encuentra**:
- ContinÃºa con el flujo normal (PrivateGPT o handoff)

---

## ğŸ“‹ RESUMEN DE LLAMADAS AL LLM

### Escenario 1: Consulta Informativa Simple

```
Usuario: "Â¿CÃ³mo cambio de carrera?"
```

**LLM Calls**:
1. `interpretar_intencion_principal()` â†’ 1 call
2. PrivateGPT RAG â†’ 1 call
   - (Opcional) ExpansiÃ³n de consulta â†’ +1 call

**Total**: **2-3 calls**

---

### Escenario 2: Consulta Operativa

```
Usuario: "Quiero cambiar de paralelo"
```

**LLM Calls**:
1. `interpretar_intencion_principal()` â†’ 1 call

**Total**: **1 call**

---

### Escenario 3: Consulta con ConfirmaciÃ³n

```
Usuario: "quiero cambiar"
LLM: needs_confirmation=true
Usuario: "sÃ­, paralelo"
```

**LLM Calls**:
1. `interpretar_intencion_principal()` â†’ 1 call (primera vez)
2. `interpretar_intencion_principal()` â†’ 1 call (despuÃ©s de confirmar)

**Total**: **2 calls**

---

## ğŸ› ï¸ CONFIGURACIÃ“N Y ARCHIVOS CLAVE

### Archivos de ConfiguraciÃ³n

1. **`handoff_config.json`**: Mapeo de categorÃ­as â†’ departamentos
2. **`data_unemi.json`**: Datos de estudiantes y perfiles
3. **`settings-docker.yaml`**: ConfiguraciÃ³n de PrivateGPT (prompts, RAG)
4. **`settings-gemini.yaml`**: ConfiguraciÃ³n especÃ­fica de Gemini

---

### Variables de Entorno

- `PRIVATEGPT_API_URL`: URL de PrivateGPT (default: `http://localhost:8001`)
- `GOOGLE_API_KEY`: API key de Gemini
- `PGPT_MODE`: Modo del LLM (`gemini`, `ollama`, etc.)

---

## ğŸ¯ PUNTOS CLAVE DEL SISTEMA

### 1. ReducciÃ³n de LLM Calls

- **Antes**: 5-7 LLM calls por consulta
- **Ahora**: 2-3 LLM calls por consulta
- **OptimizaciÃ³n**: Fusionar intenciÃ³n + confirmaciÃ³n + answer_type en 1 call

### 2. ClasificaciÃ³n Sin LLM

- `classify_with_heuristics()` usa solo reglas y `handoff_config.json`
- No requiere LLM para determinar department/channel

### 3. Filtrado AutomÃ¡tico

- Archivos temporales se filtran automÃ¡ticamente en bÃºsquedas RAG
- Limpieza automÃ¡tica al iniciar servicios

### 4. ValidaciÃ³n Robusta

- MÃºltiples capas de validaciÃ³n de `has_information`
- HeurÃ­sticas como fallback si el LLM falla

### 5. Filtrado por Rol

- PriorizaciÃ³n de documentos segÃºn rol del usuario
- Filtrado crÃ­tico en el prompt del sistema

---

## ğŸ“ NOTAS TÃ‰CNICAS

### NormalizaciÃ³n de Texto

**Archivo**: `privategpt_chat_service.py` â†’ `_normalize_text_for_llm()`

- Quita tildes y caracteres especiales antes de enviar al LLM
- Mejora la compatibilidad con modelos que tienen problemas con caracteres especiales

### Manejo de Errores

- Si PrivateGPT no estÃ¡ disponible â†’ retorna mensaje de error amigable
- Si el LLM falla â†’ usa fallbacks heurÃ­sticos
- Si el parseo falla â†’ usa heurÃ­sticas de texto

### Timeouts

- Health check: 5 segundos
- Chat completion: 60 segundos
- File ingestion: 60 segundos (timeout * 2)

---

Este documento cubre el funcionamiento completo del sistema. Â¿Hay algÃºn aspecto especÃ­fico que quieras que profundice mÃ¡s?

