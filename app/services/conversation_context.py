# app/services/conversation_context.py
"""
Manejo de contexto conversacional para preguntas de seguimiento.
Soluci√≥n industrial: usar LLM para enriquecer query con contexto.
"""
import json
from typing import List, Dict, Any, Optional
from .config import llm
from .config import guarded_invoke
import re


def build_conversation_summary(history: List[Dict[str, Any]], max_turns: int = 4) -> str:
    """
    Construye un resumen de los √∫ltimos turnos de conversaci√≥n.
    Incluye m√°s contexto para an√°lisis sem√°ntico.
    
    Args:
        history: Historial completo de conversaci√≥n
        max_turns: N√∫mero m√°ximo de turnos a incluir (default: 4, aumentado para mejor contexto)
    
    Returns:
        Resumen de conversaci√≥n en formato legible (con m√°s contexto sem√°ntico)
    """
    if not history:
        return "No hay conversaci√≥n previa."
    
    # Tomar √∫ltimos N turnos (pares usuario-bot) - m√°s turnos para mejor an√°lisis sem√°ntico
    recent = history[-max_turns*2:] if len(history) > max_turns*2 else history
    
    lines = []
    for msg in recent:
        role = msg.get("role") or msg.get("who")
        text = msg.get("content") or msg.get("text", "")
        
        if role in ("user", "student", "estudiante"):
            # Incluir preguntas completas (hasta 250 chars para mantener contexto sem√°ntico)
            lines.append(f"Usuario: {text[:250]}")
        elif role in ("bot", "assistant"):
            # Incluir m√°s contexto de las respuestas del bot (hasta 350 chars)
            # El LLM necesita ver temas y detalles para an√°lisis sem√°ntico
            text_short = text[:350] + "..." if len(text) > 350 else text
            lines.append(f"Bot: {text_short}")
    
    return "\n".join(lines)


PRONOUN_HINTS = re.compile(r"\b(eso|ese|esa|esto|as√≠|alli|all√≠|ah√≠|en ese caso|tambi√©n|lo mismo|aquello)\b", re.I)
FOLLOW_UP_HINTS = re.compile(r"\b(y|para|en|de|sobre|cuando|donde|como|que|cu√°l|cu√°les)\s+(la|el|los|las|un|una|ese|esa|eso|este|esta|esto)\b", re.I)
LOCATION_HINTS = re.compile(r"\b(para|en|de)\s+(quito|machala|santo domingo|azogues|portoviejo|santa elena|sede|ciudad)\b", re.I)

def _heuristic_needs_context(user_text: str, history: List[Dict[str, Any]]):
    if not history or len(history) < 2:
        return False, "No hay conversaci√≥n previa"
    t = (user_text or "").strip().lower()
    
    # Detectar pronombres/referencias
    if PRONOUN_HINTS.search(t):
        return True, "Pronombres/referencias detectadas"
    
    # Detectar preguntas de seguimiento que empiezan con "y", "para", etc.
    if FOLLOW_UP_HINTS.search(t):
        return True, "Pregunta de seguimiento detectada"
    
    # Detectar preguntas sobre ubicaciones/lugares espec√≠ficos (ej: "para quito")
    if LOCATION_HINTS.search(t):
        return True, "Pregunta sobre ubicaci√≥n espec√≠fica"
    
    # Detectar preguntas cortas que pueden ser de seguimiento
    if len(t) <= 3:  # ‚Äús√≠‚Äù, ‚Äúno‚Äù, ‚Äúok‚Äù
        return True, "Respuesta corta dependiente"
    
    # Detectar preguntas que empiezan con "y" (muy com√∫n en seguimientos)
    if t.startswith("y ") and len(t) > 3:
        return True, "Pregunta de seguimiento con 'y'"
    
    return False, "Pregunta auto-contenida"


def needs_context(user_text: str, conversation_history: List[Dict[str, Any]] = None) -> Dict[str, Any]:
    """
    Usa LLM para detectar si la pregunta necesita contexto conversacional previo.
    Soluci√≥n industrial sin keywords.
    
    Args:
        user_text: Texto del usuario
        conversation_history: Historial de conversaci√≥n (opcional)
    
    Returns:
        {
            "needs_context": bool,
            "confidence": "high" | "medium" | "low",
            "reason": str
        }
    """
    # Si no hay historial, no puede necesitar contexto
    if not conversation_history or len(conversation_history) < 2:
        return {
            "needs_context": False,
            "confidence": "high",
            "reason": "No hay conversaci√≥n previa"
        }
    
    # Heur√≠stica previa: solo filtrar casos OBVIOS donde NO hay contexto
    # (ej: "s√≠", "no", "ok" sin contexto previo, o primera pregunta de la conversaci√≥n)
    # Para todo lo dem√°s, dejar que el LLM haga an√°lisis sem√°ntico
    flag, reason = _heuristic_needs_context(user_text, conversation_history or [])
    # Solo usar heur√≠stica para casos MUY obvios (respuestas de 1-2 palabras sin contexto claro)
    # Para el resto, SIEMPRE invocar LLM para an√°lisis sem√°ntico
    if not flag and len(user_text.strip()) <= 2:
        # Solo filtrar respuestas muy cortas que claramente NO tienen contexto
        return {
            "needs_context": False,
            "confidence": "high",
            "reason": reason
        }
    # Para todo lo dem√°s (preguntas normales), SIEMPRE analizar con LLM

    # Construir resumen breve de conversaci√≥n
    context_summary = build_conversation_summary(conversation_history, max_turns=2)
    
    prompt = f"""Analiza SEM√ÅNTICAMENTE si esta pregunta necesita contexto de la conversaci√≥n previa para ser entendida.

IMPORTANTE: Haz an√°lisis SEM√ÅNTICO, NO solo b√∫squeda de keywords. Analiza el SIGNIFICADO y la RELACI√ìN entre la pregunta y el contexto.

CONVERSACI√ìN PREVIA:
{context_summary}

PREGUNTA ACTUAL:
"{user_text}"

AN√ÅLISIS SEM√ÅNTICO:
- INDEPENDIENTE: La pregunta tiene sentido completo por s√≠ sola, sin referencias sem√°nticas al contexto previo.
- DEPENDIENTE: La pregunta tiene una RELACI√ìN SEM√ÅNTICA con el contexto, aunque no use palabras clave obvias.

Ejemplos de relaciones SEM√ÅNTICAS que requieren contexto:
- Contexto: "Ex√°menes finales: 24-29 noviembre"
  Pregunta: "y para quito?" ‚Üí DEPENDIENTE (se refiere sem√°nticamente a fechas de ex√°menes para Quito)
- Contexto: "Cambio de paralelo requiere documentos..."
  Pregunta: "qu√© documentos necesito?" ‚Üí DEPENDIENTE (se refiere sem√°nticamente a documentos para cambio de paralelo)
- Contexto: "Matriculaci√≥n inicia en marzo"
  Pregunta: "cu√°ndo es?" ‚Üí DEPENDIENTE (se refiere sem√°nticamente a fechas de matriculaci√≥n)
- Contexto: "Asistencia m√≠nima es 70%"
  Pregunta: "y si falto m√°s?" ‚Üí DEPENDIENTE (se refiere sem√°nticamente a consecuencias de faltar m√°s)

Ejemplos de preguntas INDEPENDIENTES (sin relaci√≥n sem√°ntica con contexto):
- "¬øCu√°l es la asistencia m√≠nima?" (pregunta completa y auto-contenida)
- "¬øC√≥mo cambio mi contrase√±a?" (pregunta completa sobre tema diferente)
- "Necesito informaci√≥n sobre becas" (solicitud completa sobre tema nuevo)

REGLAS:
1. Analiza el TEMA PRINCIPAL del contexto y si la pregunta se refiere a ese tema
2. No dependas solo de keywords como "y", "para", "tambi√©n"
3. Detecta relaciones sem√°nticas aunque no haya palabras clave obvias
4. Si la pregunta ampl√≠a, especifica o contin√∫a el tema del contexto ‚Üí DEPENDIENTE
5. Si la pregunta es sobre un tema completamente diferente ‚Üí INDEPENDIENTE

Responde ESTRICTAMENTE en formato JSON:
{{
  "needs_context": true/false,
  "confidence": "high/medium/low",
  "reason": "explicaci√≥n breve de la relaci√≥n sem√°ntica detectada (o su ausencia)"
}}

JSON:"""

    try:
        response = guarded_invoke(llm, prompt).content.strip()
        
        # Limpiar markdown si existe
        if response.startswith("```json"):
            response = response.replace("```json", "").replace("```", "").strip()
        elif response.startswith("```"):
            response = response.replace("```", "").strip()
        
        result = json.loads(response)
        
        # Validar campos
        if "needs_context" not in result:
            result["needs_context"] = False
        if "confidence" not in result:
            result["confidence"] = "medium"
        if "reason" not in result:
            result["reason"] = "Evaluaci√≥n autom√°tica"
        
        print(f"üß† [Context Detection] needs={result['needs_context']}, confidence={result['confidence']}")
        print(f"   Reason: {result['reason']}")
        
        return result
        
    except Exception as e:
        print(f"‚ö†Ô∏è [Context Detection Error] {e}, asumiendo independiente")
        return {
            "needs_context": False,
            "confidence": "low",
            "reason": "Error en evaluaci√≥n"
        }


def enrich_query_with_context(
    user_text: str,
    conversation_history: List[Dict[str, Any]]
) -> str:
    """
    Enriquece la query del usuario con contexto de la conversaci√≥n previa.
    Usa LLM para resolver referencias y crear una query auto-contenida.
    
    Args:
        user_text: Query actual del usuario
        conversation_history: Historial de conversaci√≥n
    
    Returns:
        Query enriquecida con contexto (auto-contenida)
    """
    # Evaluar si necesita contexto usando LLM (solo si heur√≠stica lo sugiere)
    context_check = needs_context(user_text, conversation_history)
    
    # Si no necesita contexto, retornar query original
    if not context_check["needs_context"]:
        print(f"üîµ [Context] Query independiente, no necesita enriquecimiento")
        print(f"   Reason: {context_check['reason']}")
        return user_text
    
    # Construir resumen de conversaci√≥n
    context_summary = build_conversation_summary(conversation_history, max_turns=3)
    
    # Prompt para enriquecer query con contexto
    prompt = f"""Eres un asistente que reformula preguntas para hacerlas auto-contenidas.

CONVERSACI√ìN PREVIA:
{context_summary}

PREGUNTA ACTUAL DEL USUARIO:
"{user_text}"

TAREA:
La pregunta actual puede tener referencias al contexto previo (pronombres, referencias, etc.).
Reformula la pregunta para que sea COMPLETA y AUTO-CONTENIDA, sin necesitar el contexto.

REGLAS:
1. Reemplaza pronombres ("eso", "esto") con lo que realmente significan del contexto
2. Reemplaza referencias ("lo anterior", "tu respuesta") con el tema espec√≠fico
3. Si es una pregunta de seguimiento, incluye el tema de conversaci√≥n
4. Mant√©n el mismo sentido e intenci√≥n de la pregunta original
5. Si la pregunta ya es clara y auto-contenida, devu√©lvela sin cambios
6. Responde SOLO con la pregunta reformulada, sin explicaciones

PREGUNTA REFORMULADA:"""

    try:
        enriched_query = guarded_invoke(llm, prompt).content.strip()
        
        # Limpiar comillas si las agreg√≥ el LLM
        enriched_query = enriched_query.strip('"').strip("'").strip()
        
        print(f"üîÑ [Context Enrichment]")
        print(f"   Original: {user_text}")
        print(f"   Enriquecida: {enriched_query}")
        
        return enriched_query
        
    except Exception as e:
        print(f"‚ö†Ô∏è [Context Error] {e}, usando query original")
        return user_text






