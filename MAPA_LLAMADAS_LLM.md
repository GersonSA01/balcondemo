# ü§ñ MAPA DETALLADO DE TODAS LAS LLAMADAS AL LLM

## üìä RESUMEN EJECUTIVO

| M√©trica | Valor |
|---------|-------|
| **Total m√≥dulos que usan LLM** | 7 |
| **Total funciones que llaman LLM** | 11 |
| **Llamadas por consulta (min)** | 3 |
| **Llamadas por consulta (max)** | 10 |
| **Llamadas por consulta (promedio)** | 5-6 |
| **Tiempo por llamada LLM** | 1-2 segundos |
| **Modelo usado** | Gemini 2.5 Flash |
| **Temperatura** | 0 (determinista) |
| **Max retries** | 2 |

---

## üó∫Ô∏è MAPA COMPLETO POR M√ìDULO

### 1Ô∏è‚É£ `intent_parser.py` - 2 llamadas LLM

#### LLM Call #1: `interpretar_intencion_principal()`

```python
def interpretar_intencion_principal(texto_usuario: str) -> dict:
    prompt = f"{INTENT_SYSTEM_V2}\n\nTEXTO:\n{texto_usuario}\n"
    out = llm.invoke(prompt)  # ‚Üê LLM CALL #1
```

**Prop√≥sito**: Extraer slots de intenci√≥n estructurada en JSON
**Input**: Texto del usuario
**Output**: JSON con intent_short, acci√≥n, objeto, asignatura, etc.
**Ejemplo**:
```json
{
  "intent_short": "Solicitar cambio de paralelo para una asignatura",
  "accion": "cambiar",
  "objeto": "paralelo",
  "asignatura": "Matem√°tica",
  "detalle_libre": "por razones laborales"
}
```

**Cu√°ndo se ejecuta**:
- Stage "ready" (primera interacci√≥n)
- Despu√©s de enriquecer query con contexto (si aplica)

---

#### LLM Call #2: `_confirm_text_from_slots()`

```python
def _confirm_text_from_slots(sl: dict) -> str:
    msgs = ChatPromptTemplate.from_messages([
        ("system", sys + "\n\n" + fewshot),
        ("human", user),
    ]).format_messages()
    
    out = llm.invoke(msgs)  # ‚Üê LLM CALL #2
```

**Prop√≥sito**: Generar pregunta de confirmaci√≥n natural
**Input**: Slots de intenci√≥n (JSON)
**Output**: Pregunta en espa√±ol natural
**Ejemplo**:
- Input: `{"accion": "cambiar", "objeto": "paralelo"}`
- Output: "¬øQuieres cambiar de paralelo?"

**Cu√°ndo se ejecuta**:
- Stage "ready" despu√©s de interpretar intenci√≥n
- Para confirmar con el usuario antes de buscar

---

### 2Ô∏è‚É£ `conversation_context.py` - 2 llamadas LLM

#### LLM Call #3: `needs_context()` (JSON Evaluation)

```python
def needs_context(user_text: str, conversation_history: List[Dict]) -> Dict:
    prompt = f"""Analiza si esta pregunta necesita contexto de la conversaci√≥n previa...
    
    Responde ESTRICTAMENTE en formato JSON:
    {{
      "needs_context": true/false,
      "confidence": "high/medium/low",
      "reason": "explicaci√≥n breve"
    }}
    """
    
    response = llm.invoke(prompt)  # ‚Üê LLM CALL #3
```

**Prop√≥sito**: Detectar si la pregunta necesita contexto conversacional
**Input**: Pregunta actual + historial (√∫ltimos 2 turnos)
**Output**: JSON con evaluaci√≥n
**Ejemplo**:
```json
{
  "needs_context": true,
  "confidence": "high",
  "reason": "Usa pronombre 'eso' que se refiere a respuesta anterior"
}
```

**Cu√°ndo se ejecuta**:
- Stage "await_confirm" antes de buscar
- Solo si hay historial (‚â•2 mensajes)

---

#### LLM Call #4: `enrich_query_with_context()`

```python
def enrich_query_with_context(user_text: str, conversation_history: List[Dict]) -> str:
    prompt = f"""Reformula la pregunta para que sea COMPLETA y AUTO-CONTENIDA...
    
    CONVERSACI√ìN PREVIA:
    {context_summary}
    
    PREGUNTA ACTUAL:
    "{user_text}"
    
    PREGUNTA REFORMULADA:"""
    
    enriched_query = llm.invoke(prompt)  # ‚Üê LLM CALL #4
```

**Prop√≥sito**: Enriquecer query con contexto para hacerla auto-contenida
**Input**: Pregunta actual + resumen de conversaci√≥n
**Output**: Pregunta reformulada
**Ejemplo**:
- Input: "¬øY si falto m√°s de eso?"
- Context: "La asistencia m√≠nima es 60%"
- Output: "¬øQu√© pasa si falto m√°s del 60% de asistencia?"

**Cu√°ndo se ejecuta**:
- Solo si `needs_context = true`
- Antes de re-interpretar la intenci√≥n

---

### 3Ô∏è‚É£ `answerability.py` - 2 llamadas LLM

#### LLM Call #5: `answerability_score()` - Veredicto del Juez

```python
def answerability_score(intent_query: str, retr, k: int = 8) -> dict:
    judge_sys = (
        "Eres un juez que eval√∫a si el contexto permite responder una consulta...\n"
        "Devuelve SOLO 'yes' o 'no'."
    )
    
    msgs = ChatPromptTemplate.from_messages([
        ("system", judge_sys),
        ("human", "Consulta:\n{q}\n\nContexto (extractos):\n{c}\n\n¬øSe puede responder algo √∫til? (yes/no)")
    ]).format_messages(q=intent_query, c=sample[:6000])
    
    out = llm.invoke(msgs)  # ‚Üê LLM CALL #5
```

**Prop√≥sito**: Juez LLM que eval√∫a si el contexto recuperado puede responder
**Input**: Query + extractos de documentos recuperados (top 5)
**Output**: "yes" o "no"
**Ejemplo**:
- Query: "¬øCu√°l es la asistencia m√≠nima?"
- Context: "...se establece que el estudiante tiene permitido ausentarse en un m√°ximo del 40%..."
- Output: "yes"

**Cu√°ndo se ejecuta**:
- Por cada `planned_query` (hasta 3 veces)
- Es parte del c√°lculo de confidence score

---

#### LLM Call #6: `gen_query_variants_llm()`

```python
def gen_query_variants_llm(original_query: str, n: int = 4) -> list[str]:
    sys = (
        "Eres un experto en reformular consultas acad√©micas desde diferentes √°ngulos.\n"
        f"TAREA: Genera EXACTAMENTE {n} reformulaciones DIFERENTES..."
    )
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", sys),
        ("human", f"{original_query}")
    ])
    
    out = llm.invoke(messages)  # ‚Üê LLM CALL #6
```

**Prop√≥sito**: Generar variantes de la query para mejorar recall
**Input**: Query original
**Output**: Lista de 4 reformulaciones
**Ejemplo**:
- Input: "Solicitar cambio de paralelo"
- Output:
  1. "¬øCu√°l es el procedimiento para cambiar de paralelo?"
  2. "¬øC√≥mo gestionar el cambio de horario a otro grupo?"
  3. "Requisitos para trasladarse a otra secci√≥n"
  4. "¬øQui√©n autoriza el cambio de paralelo?"

**Cu√°ndo se ejecuta**:
- Solo si `confidence < 0.65` en answerability inicial
- Una vez por consulta (no por cada variante)

---

### 4Ô∏è‚É£ `pdf_responder.py` - 1 llamada LLM

#### LLM Call #7/#8: `responder_desde_pdfs()` - Generaci√≥n de Respuesta

```python
def responder_desde_pdfs(intent_text: str, incluir_fuente: bool = False, docs_override: list = None) -> dict:
    template = """
    Eres un asistente acad√©mico experto...
    
    Responde ESTRICTAMENTE en formato JSON con esta estructura:
    {{
      "has_information": true/false,
      "confidence": "high/medium/low",
      "answer": "tu respuesta aqu√≠"
    }}
    """
    
    prompt = ChatPromptTemplate.from_template(template)
    rag_chain = (
        {"context": lambda x: format_docs(docs), "question": RunnablePassthrough()}
        | prompt
        | llm  # ‚Üê LLM CALL #7 o #8
        | StrOutputParser()
    )
    
    respuesta_raw = rag_chain.invoke(intent_text)
    respuesta_json = json.loads(respuesta_raw)
```

**Prop√≥sito**: Generar respuesta final usando RAG + auto-evaluaci√≥n
**Input**: Query + documentos recuperados (contexto)
**Output**: JSON con respuesta y auto-evaluaci√≥n
**Ejemplo**:
```json
{
  "has_information": true,
  "confidence": "high",
  "answer": "La asistencia m√≠nima es del 60%. No se aceptan justificaciones de faltas."
}
```

**Cu√°ndo se ejecuta**:
- **LLM #7**: Nivel 1 (confidence ‚â• 0.70)
- **LLM #8**: Nivel 3 (0.42 ‚â§ confidence < 0.70)

**Auto-evaluaci√≥n incorporada**:
- `has_information`: El LLM decide si tiene info √∫til
- `confidence`: El LLM eval√∫a su propia confianza
- ‚úÖ **Zero keywords** - todo evaluaci√≥n sem√°ntica

---

### 5Ô∏è‚É£ `handoff.py` - 1 llamada LLM

#### LLM Call #9: `classify_with_llm()` (Departamento/Canal)

```python
def classify_with_llm(
    user_text: str,
    intent_short: str,
    category: Optional[str],
    subcategory: Optional[str],
    slots: Dict[str, Any]
) -> Dict[str, Any]:
    
    prompt = f"""Analiza esta solicitud de un estudiante universitario y clasif√≠cala:
    
    SOLICITUD DEL USUARIO: "{user_text}"
    INTENCI√ìN DETECTADA: "{intent_short}"
    CATEGOR√çA: "{category}"
    
    Clasifica la solicitud en JSON:
    {{
      "answer_type": "informativo | procedimental | operativo",
      "department": "acad√©mico | financiero | bienestar | administrativo | tic | biblioteca | general",
      "channel": "nombre del departamento espec√≠fico",
      "reasoning": "explicaci√≥n breve"
    }}
    
    CRITERIOS:
    - acad√©mico ‚Üí "Mesa de Ayuda Acad√©mica"
    - financiero ‚Üí "Departamento Financiero"
    - tic ‚Üí "Soporte TIC"
    ...
    
    Responde SOLO con el JSON:"""
    
    response = llm.invoke(prompt)  # ‚Üê LLM CALL #9
```

**Prop√≥sito**: Clasificar solicitud para determinar canal de derivaci√≥n correcto
**Input**: Texto original + intenci√≥n + categor√≠a + slots
**Output**: JSON con departamento y canal
**Ejemplo**:
```json
{
  "answer_type": "operativo",
  "department": "acad√©mico",
  "channel": "Mesa de Ayuda Acad√©mica",
  "reasoning": "Solicitud de cambio acad√©mico requiere validaci√≥n administrativa"
}
```

**Cu√°ndo se ejecuta**:
- Nivel 4 (sin informaci√≥n o intenci√≥n cr√≠tica)
- Antes de crear el ticket autom√°tico

---

### 6Ô∏è‚É£ `taxonomy.py` - 1 llamada LLM

#### LLM Call #10: `map_to_taxonomy()`

```python
def map_to_taxonomy(user_text: str) -> dict:
    sistema = (
        "Eres un clasificador. Debes elegir exactamente UNA ruta de taxonom√≠a "
        "que mejor corresponda a la intenci√≥n. La ruta debe ser una de la lista dada.\n"
        "Responde SOLO JSON con la clave 'ruta'. Sin explicaciones."
    )
    
    lista = "\n".join(f"- {o}" for o in opciones[:200])
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", sistema),
        ("human", f"Intenci√≥n: {user_text}\n"
                 f"Opciones (elige exactamente una):\n{lista}\n"
                 'Salida (estricta): {{"ruta":"<una opci√≥n exactamente como aparece arriba>"}}')
    ])
    
    out = llm.invoke(prompt)  # ‚Üê LLM CALL #10
```

**Prop√≥sito**: Mapear intenci√≥n a taxonom√≠a de categor√≠as
**Input**: Texto del usuario + lista de opciones de taxonom√≠a
**Output**: JSON con ruta seleccionada
**Ejemplo**:
- Input: "Cambio de paralelo"
- Opciones: ["Acad√©mico ‚Ä∫ Matriculaci√≥n", "Acad√©mico ‚Ä∫ Cambios", ...]
- Output: `{"ruta": "Acad√©mico ‚Ä∫ Cambios"}`

**Cu√°ndo se ejecuta**:
- Nivel 4 (derivaci√≥n al agente)
- Para categorizar el ticket correctamente

---

### 7Ô∏è‚É£ `retriever.py` - ‚ö†Ô∏è NO usa LLM directamente

**IMPORTANTE**: El retriever usa **embeddings pre-calculados**, NO llama al LLM en tiempo real.

```python
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
)
# ‚Üê Usa modelo local de embeddings, NO Gemini LLM
```

**Componentes del Retriever**:
1. **Dense (FAISS)**: B√∫squeda por embeddings (0 LLM calls)
2. **Sparse (BM25)**: B√∫squeda por keywords (0 LLM calls)
3. **Ensemble**: Combina ambos (0 LLM calls)
4. **Cross-Encoder**: Desactivado (muy lento)

**MultiQueryRetriever** (EXCEPCI√ìN):
```python
if MultiQueryRetriever is not None:
    mq_dense = MultiQueryRetriever.from_llm(
        retriever=dense,
        llm=llm,  # ‚Üê USA LLM para generar query variants
        prompt=ChatPromptTemplate.from_template(
            "Genera 2 reformulaciones espec√≠ficas para buscar en el reglamento:\n{question}"
        )
    )
```

**Nota**: Este componente genera variantes pero **est√° dentro del retriever**, no se cuenta por separado porque las variantes ya est√°n contadas en `gen_query_variants_llm()`.

---

## üìà TABLA DE LLAMADAS POR ESCENARIO

### Escenario 1: Consulta Simple con Info Clara (Best Case)

| # | Funci√≥n | M√≥dulo | Etapa |
|---|---------|--------|-------|
| 1 | interpretar_intencion_principal() | intent_parser | Stage ready |
| 2 | _confirm_text_from_slots() | intent_parser | Stage ready |
| 3 | answerability_score() (1x) | answerability | Retrieval |
| 4 | responder_desde_pdfs() | pdf_responder | Nivel 1 |

**TOTAL: 4 llamadas LLM**
**Tiempo estimado: 4-6 segundos**

---

### Escenario 2: Consulta con Contexto Conversacional

| # | Funci√≥n | M√≥dulo | Etapa |
|---|---------|--------|-------|
| 1 | interpretar_intencion_principal() | intent_parser | Stage ready |
| 2 | _confirm_text_from_slots() | intent_parser | Stage ready |
| 3 | needs_context() | conversation_context | Pre-retrieval |
| 4 | enrich_query_with_context() | conversation_context | Pre-retrieval |
| 5 | interpretar_intencion_principal() | intent_parser | Re-interpretaci√≥n |
| 6 | answerability_score() (1x) | answerability | Retrieval |
| 7 | responder_desde_pdfs() | pdf_responder | Nivel 1 |

**TOTAL: 7 llamadas LLM**
**Tiempo estimado: 10-14 segundos**

---

### Escenario 3: Consulta Compleja con Query Expansion

| # | Funci√≥n | M√≥dulo | Etapa |
|---|---------|--------|-------|
| 1 | interpretar_intencion_principal() | intent_parser | Stage ready |
| 2 | _confirm_text_from_slots() | intent_parser | Stage ready |
| 3 | answerability_score() (3x) | answerability | Multi-stage retrieval |
| 4 | gen_query_variants_llm() | answerability | Query expansion |
| 5 | responder_desde_pdfs() | pdf_responder | Nivel 3 |

**TOTAL: 6 llamadas LLM** (1+1+3+1=6)
**Tiempo estimado: 8-12 segundos**

---

### Escenario 4: Sin Informaci√≥n - Handoff Autom√°tico (Worst Case)

| # | Funci√≥n | M√≥dulo | Etapa |
|---|---------|--------|-------|
| 1 | interpretar_intencion_principal() | intent_parser | Stage ready |
| 2 | _confirm_text_from_slots() | intent_parser | Stage ready |
| 3 | needs_context() | conversation_context | Pre-retrieval |
| 4 | enrich_query_with_context() | conversation_context | Pre-retrieval |
| 5 | interpretar_intencion_principal() | intent_parser | Re-interpretaci√≥n |
| 6 | answerability_score() (3x) | answerability | Multi-stage (3 queries) |
| 7 | gen_query_variants_llm() | answerability | Query expansion |
| 8 | responder_desde_pdfs() | pdf_responder | Nivel 3 (rechazado) |
| 9 | classify_with_llm() | handoff | Clasificaci√≥n canal |
| 10 | map_to_taxonomy() | taxonomy | Clasificaci√≥n categor√≠a |

**TOTAL: 10 llamadas LLM** (peor caso)
**Tiempo estimado: 15-20 segundos**

---

## üéØ OPTIMIZACIONES IMPLEMENTADAS

### 1. Cach√© de Embeddings
```python
# Los embeddings de los PDFs se calculan UNA VEZ y se guardan
vectorstore.save_local(str(ruta_indice))  # Guarda embeddings en disco
# Cargas posteriores: instant√°neas
vectorstore = FAISS.load_local(str(ruta_indice), embeddings)
```

**Ahorro**: ~5-10 segundos por consulta (sin re-calcular embeddings)

### 2. L√≠mite de Query Variants
```python
variants = gen_query_variants_llm(query, n=4)  # ‚Üê Solo 4 variantes, no m√°s
```

**Ahorro**: 1 LLM call genera 4 variantes (vs 4 LLM calls)

### 3. L√≠mite de Planned Queries
```python
for i, pq in enumerate(planned_queries[:3], 1):  # ‚Üê M√°ximo 3, no 5
```

**Ahorro**: 2 menos llamadas de answerability

### 4. Cross-Encoder Desactivado
```python
FEATURE_FLAGS = {
    "cross_encoder_rerank": False,  # ‚Üê OFF (agrega 3-5 segundos)
}
```

**Ahorro**: 3-5 segundos por consulta

### 5. Auto-Evaluaci√≥n en Respuesta
```python
# Antes: 2 LLM calls (respuesta + evaluaci√≥n separada)
# Ahora: 1 LLM call (respuesta con JSON que incluye evaluaci√≥n)
{
  "has_information": true,  # ‚Üê Evaluaci√≥n incorporada
  "answer": "..."
}
```

**Ahorro**: 1 LLM call menos por respuesta

---

## üí∞ COSTOS Y QUOTAS

### Gemini 2.5 Flash (Free Tier)

| M√©trica | Valor |
|---------|-------|
| **Requests por minuto** | 10 RPM |
| **Tokens por minuto** | 1,000,000 TPM |
| **Requests por d√≠a** | 1,500 RPD |

### C√°lculo de Throughput

**Por consulta**:
- Best case: 4 LLM calls
- Worst case: 10 LLM calls
- Promedio: 6 LLM calls

**Throughput te√≥rico**:
- Con 10 RPM limit ‚Üí ~1-2 consultas por minuto
- Con 6 calls promedio ‚Üí 10/6 = 1.66 consultas/minuto
- Por hora: ~100 consultas/hora
- Por d√≠a: ~2,400 consultas/d√≠a (pero limit es 1,500 RPD)

**L√≠mite real**: ~250 consultas/d√≠a (con 6 calls/consulta)

---

## üöÄ MEJORAS FUTURAS

### 1. Parallel LLM Calls (donde sea posible)

```python
# Actual: Sequential
result1 = llm.invoke(prompt1)  # 1.5s
result2 = llm.invoke(prompt2)  # 1.5s
# Total: 3 segundos

# Mejorado: Async parallel
results = await asyncio.gather(
    llm.ainvoke(prompt1),  # 1.5s
    llm.ainvoke(prompt2),  # 1.5s
)
# Total: 1.5 segundos (paralelo)
```

**Ahorro potencial**: ~40-50% del tiempo total

### 2. Cach√© de Respuestas Frecuentes

```python
# Redis cache para preguntas comunes
cache_key = hash(query)
if cached := redis.get(cache_key):
    return cached  # ‚Üê 0 LLM calls
```

**Ahorro**: 100% para queries repetidas (e.g., "¬øCu√°l es la asistencia m√≠nima?")

### 3. Reduce Confirmation Step (Opcional)

```python
# Si la confianza en la interpretaci√≥n es muy alta (>0.95)
# Skip confirmaci√≥n y proceder directo
if interpretation_confidence > 0.95:
    # Ahorrar 2 LLM calls (confirmar + re-interpretar)
```

**Ahorro**: 2 LLM calls en ~30% de casos

### 4. Batch Processing

```python
# Procesar m√∫ltiples consultas en un solo batch
batch_results = llm.batch([prompt1, prompt2, prompt3])
```

**Ahorro**: Reduce overhead de network/auth

---


